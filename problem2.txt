To solve this problem, the author creates StreamingLLM through this paper, which is a method to make large language models handle very long inputs efficiently. Normally, models struggle with long texts because they either use too much memory by saving all past information (dense attention) or lose important context when old data is dropped (window attention). StreamingLLM solves this by keeping a few key "attention sink" tokens from the start of the text, which stabilize the model, along with recent information. They also suggest adding a special "sink token" during training to make the process even better. This approach allows models to work with long inputs without slowing down or using too much memory, making them faster and more reliable for tasks like conversations.